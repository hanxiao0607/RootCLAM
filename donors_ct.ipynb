{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from utils import utils\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "\n",
    "import utils.args_parser  as argtools\n",
    "import utils.tools as utools\n",
    "from utils.constants import Cte\n",
    "from models import deepsvdd, adcar, adar\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'data_dir': '../Data', 'batch_size': 1000, 'num_workers': 0, 'num_samples_tr': 10000, 'equations_type': 'linear', 'normalize': 'lik', 'likelihood_names': 'b_b_b_b_b_b_b_d_d_d', 'lambda_': 0.05, 'normalize_A': None}\n",
      "{'architecture': 'dgnn', 'estimator': 'elbo', 'h_dim_list_dec': [8, 8, 8, 8], 'h_dim_list_enc': [16, 16], 'z_dim': 4, 'distr_z': 'normal', 'dropout_adj_rate': 0.0, 'dropout_adj_pa_rate': 0.2, 'dropout_adj_pa_prob_keep_self': 0.0, 'residual': 1.0, 'norm_categorical': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | HVACAModule | 42.5 K\n",
      "--------------------------------------\n",
      "42.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.5 K    Total params\n",
      "0.170     Total estimated model params size (MB)\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVACAModule(\n",
      "  (_encoder_embeddings): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_module): DisjointGNN(\n",
      "    (convs): ModuleList(\n",
      "      (0): DisjointGConv()\n",
      "    )\n",
      "    (activs): ModuleList(\n",
      "      (0): Identity()\n",
      "    )\n",
      "    (dropouts): ModuleList(\n",
      "      (0): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (residuals): ModuleList(\n",
      "      (0): DisjointDense(\n",
      "        (weights): Linear(in_features=11, out_features=128, bias=False)\n",
      "        (bias): Linear(in_features=11, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_module): DisjointGNN(\n",
      "    (convs): ModuleList(\n",
      "      (0): DisjointGConv()\n",
      "      (1): DisjointGConv()\n",
      "      (2): DisjointGConv()\n",
      "      (3): DisjointGConv()\n",
      "    )\n",
      "    (activs): ModuleList(\n",
      "      (0): ReLU()\n",
      "      (1): ReLU()\n",
      "      (2): ReLU()\n",
      "      (3): Identity()\n",
      "    )\n",
      "    (dropouts): ModuleList(\n",
      "      (0): Dropout(p=0.0, inplace=False)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "      (3): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (residuals): ModuleList(\n",
      "      (0): DisjointDense(\n",
      "        (weights): Linear(in_features=11, out_features=32, bias=False)\n",
      "        (bias): Linear(in_features=11, out_features=8, bias=False)\n",
      "      )\n",
      "      (1): DisjointDense(\n",
      "        (weights): Linear(in_features=11, out_features=64, bias=False)\n",
      "        (bias): Linear(in_features=11, out_features=8, bias=False)\n",
      "      )\n",
      "      (2): DisjointDense(\n",
      "        (weights): Linear(in_features=11, out_features=64, bias=False)\n",
      "        (bias): Linear(in_features=11, out_features=8, bias=False)\n",
      "      )\n",
      "      (3): DisjointDense(\n",
      "        (weights): Linear(in_features=11, out_features=64, bias=False)\n",
      "        (bias): Linear(in_features=11, out_features=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (likelihood_z): NormalLikelihood()\n",
      "  (prob_model_x): ProbabilisticModelSCM(\n",
      "    (_decoder_embeddings): ModuleList(\n",
      "      (0): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (1): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (2): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (3): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (4): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (5): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (6): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (7): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (8): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (9): Linear(in_features=8, out_features=1, bias=False)\n",
      "      (10): Linear(in_features=8, out_features=1, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (mse): MSELoss()\n",
      ")\n",
      "Save dir: exper_test/donors_10000_linear_lik_b_b_b_b_b_b_b_d_d_d_0.05_None/vaca/dgnn_elbo_8_8_8_8_16_16_4_normal_0.0_0.2_0.0_1.0_0/adam/0.005_0.9_0.999_1.2e-06_exp_lr_0.99/0\n",
      "\n",
      "Loading from: \n",
      "exper_test/donors_10000_linear_lik_b_b_b_b_b_b_b_d_d_d_0.05_None/vaca/dgnn_elbo_8_8_8_8_16_16_4_normal_0.0_0.2_0.0_1.0_0/adam/0.005_0.9_0.999_1.2e-06_exp_lr_0.99/0/ckpt/checkpoint-epoch=497.ckpt\n",
      "Model parameters: 42504\n",
      "Results for DeepSVDD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0    0.99948   0.99809   0.99878     26710\n",
      "         1.0    0.98117   0.99476   0.98792      2671\n",
      "\n",
      "    accuracy                        0.99779     29381\n",
      "   macro avg    0.99032   0.99642   0.99335     29381\n",
      "weighted avg    0.99781   0.99779   0.99779     29381\n",
      "\n",
      "[[26659    51]\n",
      " [   14  2657]]\n",
      "AUC ROC: 0.9993011520076207\n",
      "AUC PR: 0.9974793959994017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ADAR:\n",
      "Epoch loss: 0.8520792424678802, epoch dist loss: 0.8448224663734436, epoch l2 loss: 725.677831619978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                         | 1/20 [00:07<02:26,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.8523390889167786, epoch dist loss: 0.8426611870527267, epoch l2 loss: 967.7907295227051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▍                                       | 2/20 [00:15<02:19,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.8569421581923962, epoch dist loss: 0.8468168377876282, epoch l2 loss: 1012.5320701599121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▌                                     | 3/20 [00:22<02:03,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.856270145624876, epoch dist loss: 0.8458615131676197, epoch l2 loss: 1040.8624954223633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 4/20 [00:28<01:52,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.8505942262709141, epoch dist loss: 0.8403522558510303, epoch l2 loss: 1024.1974411010742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████                                 | 5/20 [00:35<01:43,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.8574148043990135, epoch dist loss: 0.8469192534685135, epoch l2 loss: 1049.5558013916016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████▏                              | 6/20 [00:42<01:34,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss: 0.8511158637702465, epoch dist loss: 0.8407956995069981, epoch l2 loss: 1032.0158233642578\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=__doc__)\n",
    "parser.add_argument('--dataset_file', default='_params/dataset_donors_all.yaml', type=str,\n",
    "                    help='path to configuration file for the dataset')\n",
    "parser.add_argument('--model_file', default='_params/model_vaca_donors.yaml', type=str,\n",
    "                    help='path to configuration file for the dataset')\n",
    "parser.add_argument('--trainer_file', default='_params/trainer.yaml', type=str,\n",
    "                    help='path to configuration file for the training')\n",
    "parser.add_argument('--yaml_file', default='', type=str, help='path to trained model configuration')\n",
    "parser.add_argument('-d', '--dataset_dict', action=argtools.StoreDictKeyPair, metavar=\"KEY1=VAL1,KEY2=VAL2...\",\n",
    "                    help='manually define dataset configurations as string: KEY1=VALUE1+KEY2=VALUE2+...')\n",
    "parser.add_argument('-m', '--model_dict', action=argtools.StoreDictKeyPair, metavar=\"KEY1=VAL1,KEY2=VAL2...\",\n",
    "                    help='manually define model configurations as string: KEY1=VALUE1+KEY2=VALUE2+...')\n",
    "parser.add_argument('-o', '--optim_dict', action=argtools.StoreDictKeyPair, metavar=\"KEY1=VAL1,KEY2=VAL2...\",\n",
    "                    help='manually define optimizer configurations as string: KEY1=VALUE1+KEY2=VALUE2+...')\n",
    "parser.add_argument('-t', '--trainer_dict', action=argtools.StoreDictKeyPair, metavar=\"KEY1=VAL1,KEY2=VAL2...\",\n",
    "                    help='manually define trainer configurations as string: KEY1=VALUE1+KEY2=VALUE2+...')\n",
    "parser.add_argument('-s', '--seed', default=0, type=int, help='set random seed, default: random')\n",
    "parser.add_argument('-r', '--root_dir', default='', type=str, help='directory for storing results')\n",
    "parser.add_argument('--data_dir', default='', type=str, help='data directory')\n",
    "\n",
    "parser.add_argument('-i', '--is_training', default=0, type=int,\n",
    "                    help='run with training (1) or without training (0)')\n",
    "# parser.add_argument('-f', '--eval_fair', default=True, action=\"store_true\",\n",
    "#                     help='run code with counterfactual fairness experiment (only for German dataset), default: False')\n",
    "parser.add_argument('--show_results', default=0, action=\"store_true\",\n",
    "                    help='run with evaluation (1) or without(0), default: 1')\n",
    "\n",
    "parser.add_argument('--plots', default=1, type=int, help='run code with plotting (1) or without (0), default: 0')\n",
    "\n",
    "# DeepSVDD\n",
    "parser.add_argument('--training_size', default=10000, type=int, help='training size')\n",
    "parser.add_argument('--train_deepsvdd', default=0, type=int, help='train (1) or load(0) deepsvdd')\n",
    "\n",
    "parser.add_argument('--max_epoch_deepsvdd', default=1000, type=int, help='max epoch for training deepsvdd')\n",
    "parser.add_argument('--batch_size_deepsvdd', default=1024, type=int, help='batch size for training deepsvdd')\n",
    "parser.add_argument('--out_dim_deepsvdd', default=32, type=int, help='output dim for deepsvdd')\n",
    "parser.add_argument('--nu_deepsvdd', default=0.005, type=float, help='quantile for deepsvdd')\n",
    "\n",
    "# ADCAR\n",
    "parser.add_argument('--train_ADCAR', default=1, type=int, help='train (1) or load(0) ADCAR')\n",
    "parser.add_argument('--train_ADAR', default=1, type=int, help='train (1) or load(0) ADAR')\n",
    "parser.add_argument('--cost_function', default=1, type=int, help='using cost function')\n",
    "parser.add_argument('--l2_alpha', default=1e-5, type=float, help='Weight for the l2 loss')\n",
    "parser.add_argument('--device', default='cuda:0', type=str, help='Device to use')\n",
    "parser.add_argument('--max_epoch_ADCAR', default=20, type=int, help='max epoch for training ADCAR')\n",
    "parser.add_argument('--batch_size_ADCAR', default=128, type=int, help='batch size for training ADCAR')\n",
    "parser.add_argument('--learning_rate_ADCAR', default=1e-4, type=float, help='Learning rate for ADCAR')\n",
    "\n",
    "parser.add_argument('--r_ratio', default=0.0, type=float, help='R ratio for flap samples')\n",
    "\n",
    "\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# %%\n",
    "if args.yaml_file == '':\n",
    "    cfg = argtools.parse_args(args.dataset_file)\n",
    "    cfg.update(argtools.parse_args(args.model_file))\n",
    "    cfg.update(argtools.parse_args(args.trainer_file))\n",
    "else:\n",
    "    cfg = argtools.parse_args(args.yaml_file)\n",
    "if len(args.root_dir) > 0:  cfg['root_dir'] = args.root_dir\n",
    "if int(args.seed) >= 0:\n",
    "    cfg['seed'] = int(args.seed)\n",
    "\n",
    "\n",
    "# %%\n",
    "pl.seed_everything(cfg['seed'])\n",
    "utils.set_seed(cfg['seed'])\n",
    "\n",
    "if args.dataset_dict is not None: cfg['dataset']['params2'].update(args.dataset_dict)\n",
    "if args.model_dict is not None: cfg['model']['params'].update(args.model_dict)\n",
    "if args.optim_dict is not None: cfg['optimizer']['params'].update(args.optim_dict)\n",
    "if args.trainer_dict is not None: cfg['trainer'].update(args.trainer_dict)\n",
    "\n",
    "if isinstance(cfg['trainer']['gpus'], int):\n",
    "    cfg['trainer']['auto_select_gpus'] = False\n",
    "    cfg['trainer']['gpus'] = '0'\n",
    "\n",
    "cfg['dataset']['params'] = cfg['dataset']['params1'].copy()\n",
    "cfg['dataset']['params'].update(cfg['dataset']['params2'])\n",
    "\n",
    "if len(args.data_dir) > 0:\n",
    "    cfg['dataset']['params']['data_dir'] = args.data_dir\n",
    "\n",
    "print(args.dataset_dict)\n",
    "print(cfg['dataset']['params'])\n",
    "print(cfg['model']['params'])\n",
    "\n",
    "# %% Load dataset\n",
    "\n",
    "data_module = None\n",
    "\n",
    "if cfg['dataset']['name'] in Cte.DATASET_LIST:\n",
    "    from data_modules.het_scm import HeterogeneousSCMDataModule\n",
    "\n",
    "    dataset_params = cfg['dataset']['params'].copy()\n",
    "    dataset_params['dataset_name'] = cfg['dataset']['name']\n",
    "    dataset_params['num_samples_tr'] = args.training_size\n",
    "\n",
    "    data_module = HeterogeneousSCMDataModule(**dataset_params)\n",
    "\n",
    "    data_module.prepare_data()\n",
    "\n",
    "assert data_module is not None, cfg['dataset']\n",
    "\n",
    "# %% Load model\n",
    "model_vaca = None\n",
    "model_params = cfg['model']['params'].copy()\n",
    "# VACA\n",
    "if cfg['model']['name'] == Cte.VACA:\n",
    "    from models.vaca.vaca import VACA\n",
    "\n",
    "    model_params['is_heterogeneous'] = data_module.is_heterogeneous\n",
    "    model_params['likelihood_x'] = data_module.likelihood_list\n",
    "\n",
    "    model_params['deg'] = data_module.get_deg(indegree=True)\n",
    "    model_params['num_nodes'] = data_module.num_nodes\n",
    "    model_params['edge_dim'] = data_module.edge_dimension\n",
    "    model_params['scaler'] = data_module.scaler\n",
    "\n",
    "    model_vaca = VACA(**model_params)\n",
    "    model_vaca.set_random_train_sampler(data_module.get_random_train_sampler())\n",
    "# VACA with PIWAE\n",
    "elif cfg['model']['name'] == Cte.VACA_PIWAE:\n",
    "    from models.vaca.vaca_piwae import VACA_PIWAE\n",
    "\n",
    "    model_params['is_heterogeneous'] = data_module.is_heterogeneous\n",
    "\n",
    "    model_params['likelihood_x'] = data_module.likelihood_list\n",
    "\n",
    "    model_params['deg'] = data_module.get_deg(indegree=True)\n",
    "    model_params['num_nodes'] = data_module.num_nodes\n",
    "    model_params['edge_dim'] = data_module.edge_dimension\n",
    "    model_params['scaler'] = data_module.scaler\n",
    "\n",
    "    model_vaca = VACA_PIWAE(**model_params)\n",
    "    model_vaca.set_random_train_sampler(data_module.get_random_train_sampler())\n",
    "\n",
    "\n",
    "\n",
    "# MultiCVAE\n",
    "elif cfg['model']['name'] == Cte.MCVAE:\n",
    "    from models.multicvae.multicvae import MCVAE\n",
    "\n",
    "    model_params['likelihood_x'] = data_module.likelihood_list\n",
    "\n",
    "    model_params['topological_node_dims'] = data_module.train_dataset.get_node_columns_in_X()\n",
    "    model_params['topological_parents'] = data_module.topological_parents\n",
    "    model_params['scaler'] = data_module.scaler\n",
    "    model_params['num_epochs_per_nodes'] = int(\n",
    "        np.floor((cfg['trainer']['max_epochs'] / len(data_module.topological_nodes))))\n",
    "    model_vaca = MCVAE(**model_params)\n",
    "    model_vaca.set_random_train_sampler(data_module.get_random_train_sampler())\n",
    "    cfg['early_stopping'] = False\n",
    "\n",
    "# CAREFL\n",
    "elif cfg['model']['name'] == Cte.CARELF:\n",
    "    from models.carefl.carefl import CAREFL\n",
    "\n",
    "    model_params['node_per_dimension_list'] = data_module.train_dataset.node_per_dimension_list\n",
    "    model_params['scaler'] = data_module.scaler\n",
    "    model_vaca = CAREFL(**model_params)\n",
    "assert model_vaca is not None\n",
    "\n",
    "utools.enablePrint()\n",
    "\n",
    "print(model_vaca.model)\n",
    "model_vaca.summarize()\n",
    "model_vaca.set_optim_params(optim_params=cfg['optimizer'],\n",
    "                            sched_params=cfg['scheduler'])\n",
    "\n",
    "# %% Evaluator\n",
    "\n",
    "evaluator = None\n",
    "\n",
    "if cfg['dataset']['name'] in Cte.DATASET_LIST:\n",
    "    from models._evaluator import MyEvaluator\n",
    "\n",
    "    evaluator = MyEvaluator(model=model_vaca,\n",
    "                            intervention_list=data_module.train_dataset.get_intervention_list(),\n",
    "                            scaler=data_module.scaler\n",
    "                            )\n",
    "\n",
    "assert evaluator is not None\n",
    "\n",
    "model_vaca.set_my_evaluator(evaluator=evaluator)\n",
    "\n",
    "# %% Prepare training\n",
    "if args.yaml_file == '':\n",
    "    if (cfg['dataset']['name'] in [Cte.GERMAN]) and (cfg['dataset']['params3']['train_kfold'] == True):\n",
    "        save_dir = argtools.mkdir(os.path.join(cfg['root_dir'],\n",
    "                                               argtools.get_experiment_folder(cfg),\n",
    "                                               str(cfg['seed']), str(cfg['dataset']['params3']['kfold_idx'])))\n",
    "    else:\n",
    "        save_dir = argtools.mkdir(os.path.join(cfg['root_dir'],\n",
    "                                               argtools.get_experiment_folder(cfg),\n",
    "                                               str(cfg['seed'])))\n",
    "else:\n",
    "    save_dir = os.path.join(*args.yaml_file.split('/')[:-1])\n",
    "print(f'Save dir: {save_dir}')\n",
    "# trainer = pl.Trainer(**cfg['model'])\n",
    "logger = TensorBoardLogger(save_dir=save_dir, name='logs', default_hp_metric=False)\n",
    "out = logger.log_hyperparams(argtools.flatten_cfg(cfg))\n",
    "\n",
    "save_dir_ckpt = argtools.mkdir(os.path.join(save_dir, 'ckpt'))\n",
    "ckpt_file = argtools.newest(save_dir_ckpt)\n",
    "callbacks = []\n",
    "if args.is_training == 1:\n",
    "\n",
    "    checkpoint = ModelCheckpoint(period=1,\n",
    "                                 monitor=model_vaca.monitor(),\n",
    "                                 mode=model_vaca.monitor_mode(),\n",
    "                                 save_top_k=1,\n",
    "                                 save_last=True,\n",
    "                                 filename='checkpoint-{epoch:02d}',\n",
    "                                 dirpath=save_dir_ckpt)\n",
    "\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    if cfg['early_stopping']:\n",
    "        early_stopping = EarlyStopping(model_vaca.monitor(), mode=model_vaca.monitor_mode(), min_delta=0.0,\n",
    "                                       patience=50)\n",
    "        callbacks.append(early_stopping)\n",
    "\n",
    "    if ckpt_file is not None:\n",
    "        print(f'Loading model training: {ckpt_file}')\n",
    "        trainer = pl.Trainer(logger=logger, callbacks=callbacks, resume_from_checkpoint=ckpt_file,\n",
    "                             **cfg['trainer'])\n",
    "    else:\n",
    "\n",
    "        trainer = pl.Trainer(logger=logger, callbacks=callbacks, **cfg['trainer'])\n",
    "\n",
    "    # %% Train\n",
    "\n",
    "    trainer.fit(model_vaca, data_module)\n",
    "    # save_yaml(model.get_arguments(), file_path=os.path.join(save_dir, 'hparams_model.yaml'))\n",
    "    argtools.save_yaml(cfg, file_path=os.path.join(save_dir, 'hparams_full.yaml'))\n",
    "    # %% Testing\n",
    "\n",
    "else:\n",
    "    # %% Testing\n",
    "    trainer = pl.Trainer()\n",
    "    print('\\nLoading from: ')\n",
    "    print(ckpt_file)\n",
    "\n",
    "    model_vaca = model_vaca.load_from_checkpoint(ckpt_file, **model_params)\n",
    "    evaluator.set_model(model_vaca)\n",
    "    model_vaca.set_my_evaluator(evaluator=evaluator)\n",
    "\n",
    "    if cfg['model']['name'] in [Cte.VACA_PIWAE, Cte.VACA, Cte.MCVAE]:\n",
    "        model_vaca.set_random_train_sampler(data_module.get_random_train_sampler())\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model_vaca.parameters())\n",
    "params = int(sum([np.prod(p.size()) for p in model_parameters]))\n",
    "\n",
    "print(f'Model parameters: {params}')\n",
    "model_vaca.eval()\n",
    "model_vaca.freeze()  # IMPORTANT\n",
    "\n",
    "if args.show_results:\n",
    "    output_valid = model_vaca.evaluate(dataloader=data_module.val_dataloader(),\n",
    "                                       name='valid',\n",
    "                                       save_dir=save_dir,\n",
    "                                       plots=False)\n",
    "    output_test = model_vaca.evaluate(dataloader=data_module.test_dataloader(),\n",
    "                                      name='test',\n",
    "                                      save_dir=save_dir,\n",
    "                                      plots=args.plots)\n",
    "    output_valid.update(output_test)\n",
    "\n",
    "    output_valid.update(argtools.flatten_cfg(cfg))\n",
    "    output_valid.update({'ckpt_file': ckpt_file,\n",
    "                         'num_parameters': params})\n",
    "\n",
    "    with open(os.path.join(save_dir, 'output.json'), 'w') as f:\n",
    "        json.dump(output_valid, f)\n",
    "    print(f'Experiment folder: {save_dir}')\n",
    "    \n",
    "thres_n, thres_ab, df_train, df_valid, df_test = utils.split_dataset(data_module, name=cfg['dataset']['name'], \\\n",
    "                                                                     training_size=args.training_size,\n",
    "                                                                     seed=args.seed)\n",
    "\n",
    "if cfg['dataset']['name'] == 'loan':\n",
    "    input_dim = data_module.train_dataset.X0.shape[-1]\n",
    "elif cfg['dataset']['name'] == 'adult':\n",
    "    input_dim = data_module.train_dataset.X0.shape[-1] - 4\n",
    "elif cfg['dataset']['name'] == 'donors':\n",
    "    input_dim = data_module.train_dataset.X0.shape[-1] - 1\n",
    "else:\n",
    "    NotImplementedError\n",
    "\n",
    "model_deepsvdd = deepsvdd.DeepSVDD(input_dim=input_dim, out_dim=args.out_dim_deepsvdd,\n",
    "                                   batch_size=args.batch_size_deepsvdd, nu=args.nu_deepsvdd,\n",
    "                                   max_epoch=args.max_epoch_deepsvdd, data=data_module.dataset_name,\n",
    "                                   device=args.device)\n",
    "\n",
    "if cfg['dataset']['name'] == 'loan':\n",
    "    train_X = data_module.scaler.transform(data_module.train_dataset.X0)\n",
    "    valid_X = data_module.scaler.transform(data_module.valid_dataset.X0)\n",
    "    test_X = data_module.scaler.transform(data_module.test_dataset.X0)\n",
    "elif cfg['dataset']['name'] == 'adult':\n",
    "    train_X = data_module.scaler.transform(data_module.train_dataset.X0)[:, :-4]\n",
    "    valid_X = data_module.scaler.transform(data_module.valid_dataset.X0)[:, :-4]\n",
    "    test_X = data_module.scaler.transform(data_module.test_dataset.X0)[:, :-4]\n",
    "elif cfg['dataset']['name'] == 'donors':\n",
    "    train_X = data_module.scaler.transform(data_module.train_dataset.X0)[:, :-1]\n",
    "    valid_X = data_module.scaler.transform(data_module.valid_dataset.X0)[:, :-1]\n",
    "    test_X = data_module.scaler.transform(data_module.test_dataset.X0)[:, :-1]\n",
    "else:\n",
    "    NotImplementedError\n",
    "\n",
    "if args.train_deepsvdd:\n",
    "    print('Training DeepSVDD:')\n",
    "    model_deepsvdd.train_DeepSVDD(train_X, valid_X)\n",
    "\n",
    "print('Results for DeepSVDD:')\n",
    "model_deepsvdd.load_model()\n",
    "model_deepsvdd.get_R(train_X)\n",
    "if cfg['dataset']['name'] == 'donors':\n",
    "    lst_dist, lst_pred = model_deepsvdd.predict(test_X, label=df_test['is_exciting'].values, result=1)\n",
    "else:\n",
    "    lst_dist, lst_pred = model_deepsvdd.predict(test_X, label=df_test['label'].values, result=1)\n",
    "\n",
    "if cfg['dataset']['name'] == 'loan':\n",
    "    out_dim = 6\n",
    "elif cfg['dataset']['name'] == 'adult':\n",
    "    out_dim = 3\n",
    "elif cfg['dataset']['name'] == 'donors':\n",
    "    out_dim = 3\n",
    "else:\n",
    "    NotImplementedError\n",
    "\n",
    "\n",
    "model_adar = adar.ADAR(input_dim, out_dim, model_deepsvdd, model_vaca, data_module,\n",
    "                       alpha=args.l2_alpha, batch_size=args.batch_size_ADCAR, max_epoch=args.max_epoch_ADCAR,\n",
    "                       device=args.device, data=cfg['dataset']['name'], cost_f=args.cost_function,\n",
    "                       R_ratio=args.r_ratio, lr=args.learning_rate_ADCAR)\n",
    "                       # device=args.device, data=cfg['dataset']['name'], cost_f=args.cost_function, R_ratio=i)\n",
    "x_train, u_train, x_valid, u_valid, x_test, u_test, df = utils.prepare_adcar_training_data(df_test, lst_pred, data_module,\n",
    "                                                                                           cfg['dataset']['name'])\n",
    "if args.train_ADAR:\n",
    "    print('Training ADAR:')\n",
    "    model_adar.train_ADAR(x_train, u_train, x_valid, u_valid)\n",
    "print('Results for ADAR:')\n",
    "# model_adar.predict(x_train, u_train)\n",
    "model_adar.predict(x_test, u_test, thres_n=thres_n)\n",
    "\n",
    "print('-'*50)\n",
    "model_adcar = adcar.ADCAR(input_dim, out_dim, model_deepsvdd, model_vaca, data_module,\n",
    "                       alpha=args.l2_alpha, batch_size=args.batch_size_ADCAR, max_epoch=args.max_epoch_ADCAR,\n",
    "                       device=args.device, data=cfg['dataset']['name'], cost_f=args.cost_function, R_ratio=args.r_ratio)\n",
    "                       # device=args.device, data=cfg['dataset']['name'], cost_f=args.cost_function, R_ratio=i)\n",
    "\n",
    "if args.train_ADCAR:\n",
    "    print('Training ADCAR:')\n",
    "    model_adcar.train_ADCAR(x_train, u_train, x_valid, u_valid)\n",
    "print('Results for ADCAR:')\n",
    "# model_adcar.predict(x_train, u_train)\n",
    "model_adcar.predict(x_test, u_test, thres_n=thres_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADCAR\n",
    "parser.add_argument('--train_ADCAR', default=1, type=int, help='train (1) or load(0) ADCAR')\n",
    "parser.add_argument('--train_ADAR', default=1, type=int, help='train (1) or load(0) ADAR')\n",
    "parser.add_argument('--cost_function', default=1, type=int, help='using cost function')\n",
    "parser.add_argument('--l2_alpha', default=1e-5, type=float, help='Weight for the l2 loss')\n",
    "parser.add_argument('--device', default='cuda:0', type=str, help='Device to use')\n",
    "parser.add_argument('--max_epoch_ADCAR', default=20, type=int, help='max epoch for training ADCAR')\n",
    "parser.add_argument('--batch_size_ADCAR', default=128, type=int, help='batch size for training ADCAR')\n",
    "parser.add_argument('--learning_rate_ADCAR', default=1e-4, type=float, help='Learning rate for ADCAR')\n",
    "\n",
    "parser.add_argument('--r_ratio', default=0.0, type=float, help='R ratio for flap samples')\n",
    "index = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta_adar, _,_, df_adar = model_adar.get_result(x_test[index], u_test[index])\n",
    "x_theta_adcar, _,_, df_adcar = model_adcar.get_result(x_test[index], u_test[index])\n",
    "index += 1\n",
    "df_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta_adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_theta_adcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
